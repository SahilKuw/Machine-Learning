{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM9hs3cwW86Wu8vei7hiPLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryantuckman/Machine-Learning/blob/main/tensorflow_gluon_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsPvoAJ7lf4P"
      },
      "source": [
        "## **Overview**\n",
        "\n",
        "We will be working with the [Gluon dataset](https://github.com/rabah-khalek/TF_tutorials/tree/master/PseudoData). A gluon is an elementary particle which gives rise to the strong force, which is the force responsible for binding together the protons and neutrons together in atomic nuclei.\n",
        "\n",
        "The gluon is exchanged by other elementary particles in the proton and neutron called quarks. These quarks and gluons are more generally referred to as \"partons\".\n",
        "\n",
        "Because of certain physical properties governing the partons, they cannot be observed by expermiments as free particles, and their distribution within the proton and neutron cannot be calculated analytically. Instead, their distribution must be inferred from data collected by high energy particle colliders like the Large Hadron Collider (LHC) at CERN.\n",
        "\n",
        "A fast-moving proton can be described by the parton distribution function (PDF) $f(x)$, which gives the probability density of finding a parton of type $f$ (a specific quark type or gluon) carrying a fraction $x$ of the proton momentum. \n",
        "\n",
        "The pseudodata used in this tutorial is generated from a gluon distribution function $g(x)$, which was determined in an analysis by the [NNPDF collaboration](http://nnpdf.mi.infn.it), aiming to extract the structure of the proton using contemporary machine learning methods. More specifically, NNPDF determines PDFs using neural networks as a minimally biased modeling tool, trained using Genetic Algorithms (and more recently stochastic gradient descent). \n",
        "\n",
        "We will consider this pseudodata to be the *truth* that we're trying to *discover*. What we will actually fit is the *smeared truth*, where Gaussian noise is added to simulate more realistic data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXhhjOlklf4Q"
      },
      "source": [
        "## **Importing the Gluon data set with Pandas**\n",
        "\n",
        "There are four separate datasets each containing a total of 1000 gluon PDF predictions computed between $x=[x_{min},1]$; where:\n",
        "- $x_{min} = 10^{-3}$ for `filename1`\n",
        "- $x_{min} = 10^{-4}$ for `filename2`\n",
        "- $x_{min} = 10^{-5}$ for `filename3`\n",
        "- $x_{min} = 10^{-6}$ for `filename4`\n",
        "  \n",
        "Below we use Pandas to import all datapoints from text files. We then use `sklearn` to split the data randomly into 800 points for training and 200 points for testing. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7pqy4uQlf4R",
        "outputId": "1ceb0ed8-c53e-4aba-9197-734193ea39e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data parsing is done!\n"
          ]
        }
      ],
      "source": [
        "# Import relevant python modules\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# The datasets needed are computed by the `ComputeGluon.py` script in PseudoData\n",
        "filename1='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-3.dat' \n",
        "filename2='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-4.dat' \n",
        "filename3='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-5.dat' \n",
        "filename4='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-6.dat' \n",
        "\n",
        "# Headers to skip\n",
        "lines_to_skip = 5\n",
        "\n",
        "# Defining the columns (cv = central value, sd = standard deviation)\n",
        "columns=[\"x\", \"gluon_cv\", \"gluon_sd\"]\n",
        "\n",
        "# Loading data from txt file\n",
        "# Change filename1 to another filename for data that extends to lower x \n",
        "# (see exercises at the bottom of this notebook)\n",
        "df = pd.read_csv(filename1, \n",
        "                 sep=\"\\s+\", \n",
        "                 skiprows=lines_to_skip, \n",
        "                 usecols=[0,1,2], \n",
        "                 names=columns)\n",
        "\n",
        "# Splitting data randomly to train and test using the sklearn library\n",
        "df_train, df_test = train_test_split(df,test_size=0.2,random_state=42)\n",
        "\n",
        "# Sort the split data according to their x values\n",
        "df_train = df_train.sort_values(\"x\")\n",
        "df_test = df_test.sort_values(\"x\")\n",
        "\n",
        "print(\"Data parsing is done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0B_rcDHXo63D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2O4__DjoiRX",
        "outputId": "08e5d052-4648-41b4-8643-5d9acc93048c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            x  gluon_cv  gluon_sd\n",
            "0    0.001000  4.480675  0.523964\n",
            "1    0.001007  4.478519  0.520743\n",
            "2    0.001014  4.476361  0.517539\n",
            "3    0.001021  4.474202  0.514349\n",
            "4    0.001028  4.472040  0.511176\n",
            "..        ...       ...       ...\n",
            "993  0.952796 -0.001109  0.003990\n",
            "994  0.959401 -0.000878  0.003423\n",
            "996  0.972747 -0.000634  0.002707\n",
            "997  0.979490 -0.000494  0.002179\n",
            "999  0.993116 -0.000144  0.000697\n",
            "\n",
            "[800 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwQaJA28lf4V"
      },
      "source": [
        "## **Why do we scale the input of a Neural Network?**\n",
        "\n",
        "Given the use of small weights in the model and the use of error between predictions and expected values, the scale of inputs and outputs used to train the model are an important factor. Unscaled input variables can result in a slow or unstable learning process, whereas unscaled target variables on regression problems can result in exploding gradients causing the learning process to fail.\n",
        "\n",
        "The input variables are those that the network takes on the input or visible layer in order to make a prediction. In this case we simply have one input variable: x.\n",
        "\n",
        "A good rule of thumb is that input variables should be small values, probably in the range of 0 to 1 or standardized with a zero mean and a standard deviation of one.\n",
        "\n",
        "Whether input variables require scaling depends on the specifics of your problem and of each variable. In principle, since the x values we consider here already range from 0 to 1, we most likely do not need to scale the neural network input. For learning purposes, we will do it anyway."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYmCm8Mslf4W",
        "outputId": "93649775-fe82-46fd-a1b9-bbbf4be512e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StandardScaler()"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "# Import relevant python modules\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Shaping the x values for the scaler\n",
        "train_inputs = df_train['x'].to_numpy().reshape((-1,1))\n",
        "test_inputs = df_test['x'].to_numpy().reshape((-1,1))\n",
        "\n",
        "#Scaling input features to help the minimizer\n",
        "train_scaler = StandardScaler()\n",
        "train_scaler.fit(train_inputs)\n",
        "test_scaler = StandardScaler()\n",
        "test_scaler.fit(test_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJr7VNShlf4X"
      },
      "source": [
        "## **Building a Neural Network using TensorFlow**\n",
        "\n",
        "The idea here is to fit the gluon distribution (which cannot be calculated analytically) using a neural network.\n",
        "\n",
        "We will build the neural network of 1 input, 1 hidden layer, and 1 output in two ways:\n",
        "- By using the high-level Keras API (TF_Model)\n",
        "- By constructing a custom NN (USER_Model)\n",
        "\n",
        "We consider a single layered neural network to be sufficient for our exercise here, but feel free to experiment with *deeper* architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZT1EthiBlf4Y"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "##################################################################\n",
        "# Building NN from the Keras API (layers.Dense)\n",
        "##################################################################\n",
        "\n",
        "class TF_Model(keras.Model):\n",
        "    \n",
        "    def __init__(self,n_features,n_neurons,n_outputs,name=None):\n",
        "        \n",
        "        super(TF_Model, self).__init__(name=name)\n",
        "        #tf.random.set_seed(42) # Set seed for parameter initialization\n",
        "        \n",
        "        # Hidden layer 1\n",
        "        self.hidden1 = layers.Dense(n_neurons, activation='sigmoid',input_shape=[n_features])\n",
        "        # Output layer\n",
        "        self.output1 = layers.Dense(n_outputs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \n",
        "        # Evaluate hidden layer\n",
        "        x = self.hidden1(x)\n",
        "        # Evaluate and return output\n",
        "        return self.output1(x)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np \n",
        "\n",
        "##################################################################\n",
        "# Building NN from the PyTorch API (nn.Linear)\n",
        "##################################################################\n",
        "\n",
        "class Torch_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_features, n_neurons, n_outputs):\n",
        "\n",
        "        super(Torch_Model, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(n_features, n_neurons) # Hidden Layer 1\n",
        "        self.sig1 = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(n_neurons, n_outputs) # Output Layer\n",
        "\n",
        "    # Evaluates and returns output\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.sig1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zabn3qogINB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nPRalijwlf4Z"
      },
      "outputs": [],
      "source": [
        "##################################################################\n",
        "# Building a custom NN layer class\n",
        "# self.w = weights\n",
        "# self.b = biases\n",
        "##################################################################\n",
        "\n",
        "class NN_Layer(keras.Model):\n",
        "    \n",
        "    def __init__(self, n_inputs, n_outputs):\n",
        "        \n",
        "        super().__init__()\n",
        "        #tf.random.set_seed(42) # Set seed for parameter initialization\n",
        "        \n",
        "        # Initialize weights and biases\n",
        "        # A truncated normal is used to prevent large starting weights, slowing down training\n",
        "        self.w = tf.Variable(tf.random.truncated_normal([n_inputs, n_outputs]), name='w')\n",
        "        self.b = tf.Variable(tf.zeros([n_outputs]), name='b')\n",
        "        \n",
        "    def __call__(self, x):\n",
        "    \n",
        "        # Compute and return the output of NN layer\n",
        "        y = tf.matmul(x, self.w) + self.b\n",
        "        \n",
        "        return y\n",
        "\n",
        "##################################################################\n",
        "# Defining our model from custom NN layers\n",
        "##################################################################\n",
        "class USER_Model(keras.Model):\n",
        "    \n",
        "    def __init__(self,n_features,n_hidden,n_outputs, name=None):\n",
        "        super(USER_Model, self).__init__(name=name)\n",
        "        \n",
        "        # Build hidden layer\n",
        "        self.hidden1 = NN_Layer(n_features,n_hidden)\n",
        "        # Build output layer\n",
        "        self.output1 = NN_Layer(n_hidden,n_outputs)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        \n",
        "        # Evaluate hidden layer\n",
        "        x = self.hidden1(x)\n",
        "        # Activate hidden layer\n",
        "        x = tf.nn.sigmoid(x)\n",
        "        # Return output\n",
        "        return self.output1(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nT8Z4Mjlf4b"
      },
      "source": [
        "## **Data modeling and fitting**\n",
        "\n",
        "\n",
        "The next few cells contain the bulk of the minimization routine.\n",
        "\n",
        "\n",
        "### **Chi-square**\n",
        "\n",
        "One of the most frequently occurring problems in high-energy physics is to compare an observed distribution with a prediction, for example from a simulation. Indeed, an analysis might be designed to extract some physical parameter from the simulation or prediction which best fits the data.  \n",
        "\n",
        "For data points with Gaussian errors, the likelihood of a gaussian distribution is:\n",
        "\n",
        "$$ L = \\prod_i \\frac{1}{\\sqrt(2\\pi\\sigma^2)} e^{-\\frac{(m_i - d_i)^2}{2\\sigma_i^2}}$$\n",
        "\n",
        "hence the following [MLE](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) (maximum (log) likelihood estimation) gives:\n",
        "\n",
        "$$ \\chi^2 = \\sum_i^N \\left(\\frac{m_i - d_i}{\\sigma_i} \\right)^2$$\n",
        "\n",
        "where $m_i$ is the model evaluated at point $x_i$, $d_i$ is the data, and $\\sigma_i$ is the uncertainty on the data at this point.\n",
        "\n",
        "The chi-square expresses the deviation of the observed data from the fit, weighted inversely by the uncertainties in the individual points. The chi-square can be either used to test how well a particular model describes the data or, if the prediction is a function of some parameters then the optimal values of the parameters can be found by minimizing the chi-square.\n",
        "\n",
        "The main pitfall here is that the purely Gaussian case is in fact rather rare, usually because the data points come from Poisson-distributed numbers of events which are not well approximated by Gaussian distributions. Using a standard chi-square approach in such cases leads to biased estimates of both the parameters and their uncertainties.\n",
        "\n",
        "Nevertheless we will assume here that our datapoints' errors are Gaussian, and we will use the chi-square as the loss function in this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQFOSt0Rlf4c"
      },
      "source": [
        "## **Prepare Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "OJSdb8y8lf4c"
      },
      "outputs": [],
      "source": [
        "# Number of training instances\n",
        "N_train = len(df_train[\"x\"])\n",
        "# Number of test instances\n",
        "N_test = len(df_test[\"x\"])\n",
        "\n",
        "# Scale training inputs\n",
        "train_x = np.array(train_scaler.transform(train_inputs)).reshape(N_train,1)\n",
        "# Convert training inputs to tensor\n",
        "train_x = tf.convert_to_tensor(train_x,dtype='float32')\n",
        "\n",
        "# Gather data labels (i.e. measurements)\n",
        "train_y = np.array(df_train[\"gluon_cv\"]).reshape(N_train,1)\n",
        "# Gather data uncertainties \n",
        "train_sigma = np.array(df_train[\"gluon_sd\"]).reshape(N_train,1)\n",
        "# Add noise to data\n",
        "np.random.seed(42)\n",
        "train_y += np.random.normal(0, train_sigma)\n",
        "# Convert data and uncertainties to tensors\n",
        "train_y = tf.convert_to_tensor(train_y,dtype='float32')\n",
        "train_sigma = tf.convert_to_tensor(train_sigma,dtype='float32')\n",
        "\n",
        "# Do the same procedure as above with test set\n",
        "test_x = np.array(test_scaler.transform(test_inputs)).reshape(N_test,1)\n",
        "test_y = np.array(df_test[\"gluon_cv\"]).reshape(N_test,1)\n",
        "test_sigma = np.array(df_test[\"gluon_sd\"]).reshape(N_test,1)\n",
        "test_y += np.random.normal(0, test_sigma)\n",
        "test_x = tf.convert_to_tensor(test_x,dtype='float32')\n",
        "test_y = tf.convert_to_tensor(test_y,dtype='float32')\n",
        "test_sigma = tf.convert_to_tensor(test_sigma,dtype='float32')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KbQstfwMlf4d"
      },
      "source": [
        "## **Prepare Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ2GUQsnlf4h"
      },
      "outputs": [],
      "source": [
        "# Number of neurons in input layer (= number of features = 1, which is momentum fraction x)\n",
        "N_features = 1\n",
        "# Number of neurons in hidden layer\n",
        "N_hidden = 5\n",
        "# Number of neurons in output layer\n",
        "N_output = 1\n",
        "\n",
        "# Initialize model\n",
        "model = TF_Model(N_features,N_hidden,N_output,name='my_TF_model')\n",
        "#model = USER_Model(N_features,N_hidden,N_output,name='my_custom_model')\n",
        "\n",
        "# User-defined loss function (chi-squared)\n",
        "def loss_function(predictions,labels,label_errors):\n",
        "    return tf.reduce_sum(tf.math.square((labels-predictions)/label_errors))\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)\n",
        "\n",
        "# Define tensorflow function for training step (computing and applying gradients)\n",
        "@tf.function\n",
        "def train_step(inputs, labels, label_errors):\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(inputs)\n",
        "        loss = loss_function(predictions,labels,label_errors)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    \n",
        "    return loss\n",
        "    \n",
        "# Define tensorflow function for computing loss\n",
        "@tf.function\n",
        "def compute_loss(inputs, labels, label_errors):\n",
        "    \n",
        "    predictions = model(inputs)\n",
        "    loss = loss_function(predictions,labels,label_errors)\n",
        "    \n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Plotting script**"
      ],
      "metadata": {
        "id": "I2it0zRWAMb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot():\n",
        "  %matplotlib inline\n",
        "  # Plot results\n",
        "  fig = plt.figure(figsize=(15,10))\n",
        "  ax = plt.subplot(111)\n",
        "  ax.plot(df_train[\"x\"],df_train[\"gluon_cv\"],color='darkblue',label='$t_{g}^{train}$', lw=3)\n",
        "  #ax.plot(df_test[\"x\"],df_test[\"gluon_cv\"],color='darkgreen',label='$t_{g}^{test}$',alpha=0.3)\n",
        "\n",
        "  #ax.scatter(df_train[\"x\"],train_y.numpy(), marker=\"o\", color='darkblue',alpha=1.,label='$d_{g}^{train}$')\n",
        "  #ax.scatter(df_test[\"x\"],test_y.numpy(),marker=\"o\",  color = 'darkgreen',alpha=1.,label='$d_{g}^{test}$')\n",
        "\n",
        "  ax.errorbar(df_train[\"x\"],train_y.numpy(),yerr=train_sigma.numpy(),fmt='.',color='darkblue',label='$d_{g}^{train}$',alpha=0.3)\n",
        "  #ax.errorbar(df_test[\"x\"],test_y.numpy(),yerr=test_sigma.numpy(),fmt='.',color='darkgreen',label='$d_{g}^{test}$',alpha=0.5)\n",
        "\n",
        "  ax.plot(df_train[\"x\"],prediction_values.numpy(),color='red',label='$NN_{g}$', lw=3)\n",
        "  ax.fill_between(df_train[\"x\"],df_train[\"gluon_cv\"]+df_train[\"gluon_sd\"],\n",
        "                  df_train[\"gluon_cv\"]-df_train[\"gluon_sd\"] ,color='blue', \n",
        "                  alpha=0.1, label='$\\sigma_{g}$')\n",
        "\n",
        "  #df_train.plot(kind='line',x='x',y=['gluon_cv'], yerr='gluon_sd',color=['red'], ax=ax)\n",
        "\n",
        "  ax.text(0.1,0.3,r'Epoch = %d'%(epoch),fontsize=20,transform=ax.transAxes)\n",
        "  ax.text(0.1,0.2,r'$\\chi^2_{\\rm training}/N_{\\rm data} = %.2f$'%(training_chi2),fontsize=20,transform=ax.transAxes)\n",
        "  ax.text(0.1,0.1,r'$\\chi^2_{\\rm test}/N_{\\rm data} = %.2f$'%(test_chi2),fontsize=20,transform=ax.transAxes)\n",
        "\n",
        "  # Plot settings\n",
        "  ax.set_xscale('log')\n",
        "  ax.set_ylabel(r'$g(x)$',fontsize=45)\n",
        "  ax.set_xlabel(r'$x$',fontsize=45)\n",
        "  ax.legend(loc='best',fontsize=30,frameon=False)\n",
        "  ax.tick_params(which='both',direction='in',labelsize=30)\n",
        "  ax.tick_params(which='major',length=10)\n",
        "  ax.tick_params(which='minor',length=5)\n",
        "  plt.tight_layout()\n",
        "  display.clear_output(wait=True)\n",
        "  display.display(plt.gcf())\n"
      ],
      "metadata": {
        "id": "VYbzQ8dk_M7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoZZFusrlf4l"
      },
      "source": [
        "## **Train and Plot Neural Network**\n",
        "\n",
        "- $t_{g}^{train}$ represents the truth value of the training set, i.e the values taken from the txt file.\n",
        "- $d_{g}^{train}$ are the smeared data we actually fit.\n",
        "- $NN_{g}$ is the output of our Neural Network.\n",
        "\n",
        "You can uncomment several other information like: $d_{g}^{test}$, the error bars used, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MY1_UnEalf4l",
        "outputId": "9e3e3731-df2f-4a1a-8447-28e3d9b57235",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Chi2/Npts: 409.46, Test Chi2/Npts: 425.98\n",
            "Epoch 1000, Chi2/Npts: 1.57, Test Chi2/Npts: 1.69\n",
            "Epoch 2000, Chi2/Npts: 1.41, Test Chi2/Npts: 1.58\n",
            "Epoch 3000, Chi2/Npts: 1.28, Test Chi2/Npts: 1.56\n",
            "Epoch 4000, Chi2/Npts: 1.20, Test Chi2/Npts: 1.55\n",
            "Epoch 5000, Chi2/Npts: 1.14, Test Chi2/Npts: 1.53\n",
            "Epoch 6000, Chi2/Npts: 1.10, Test Chi2/Npts: 1.50\n",
            "Epoch 7000, Chi2/Npts: 1.07, Test Chi2/Npts: 1.51\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-48310862b228>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sigma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mtemplate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch {:d}, Chi2/Npts: {:.2f}, Test Chi2/Npts: {:.2f}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    952\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rc\n",
        "from IPython import display\n",
        "import time\n",
        "plt.rc('font', **{'family': 'sans-serif', 'sans-serif': ['Helvetica']})\n",
        "plt.rc('text', usetex=False)\n",
        "plt.rc('xtick', labelsize=15)\n",
        "plt.rc('ytick', labelsize=15)\n",
        "\n",
        "# Number of training epochs\n",
        "training_epochs = 20000\n",
        "# Epoch intervals to print\n",
        "epoch_it = 1000\n",
        "\n",
        "VERBOSITY=False #choose True to plot the NN every iteration\n",
        "\n",
        "for epoch in range(training_epochs+1):\n",
        "    \n",
        "    train_loss = train_step(train_x, train_y, train_sigma)\n",
        "    test_loss = compute_loss(test_x, test_y, test_sigma)\n",
        "    \n",
        "    template = 'Epoch {:d}, Chi2/Npts: {:.2f}, Test Chi2/Npts: {:.2f}'\n",
        "    if epoch%epoch_it==0:\n",
        "\n",
        "        # Compute predictions and chi2 with latest model parameters\n",
        "        prediction_values = model(train_x)\n",
        "        training_chi2 = loss_function(prediction_values,train_y,train_sigma).numpy()/N_train\n",
        "        test_prediction_values = model(test_x)\n",
        "        test_chi2 = loss_function(test_prediction_values,test_y,test_sigma).numpy()/N_test\n",
        "        \n",
        "        if not VERBOSITY:\n",
        "          print(template.format(epoch, training_chi2, test_chi2))\n",
        "          if epoch==training_epochs:\n",
        "            plot();\n",
        "            display.clear_output(wait=True)\n",
        "        else:\n",
        "          plot();\n",
        "          display.clear_output(wait=True)\n"
      ]
    }
  ]
}