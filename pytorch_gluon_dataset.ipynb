{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQmkcSqPCD0eBufX9MfgbN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ryantuckman/Machine-Learning/blob/main/pytorch_gluon_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6x5yyTGp6fe",
        "outputId": "48f9ddcf-f255-418c-e4b8-d8e033f4e61f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchviz\n",
            "  Downloading torchviz-0.0.2.tar.gz (4.9 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchviz) (1.12.1+cu113)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (from torchviz) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchviz) (4.1.1)\n",
            "Building wheels for collected packages: torchviz\n",
            "  Building wheel for torchviz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchviz: filename=torchviz-0.0.2-py3-none-any.whl size=4150 sha256=1c09d4c9d7ced5f82658ac421638d912ca0bafcd13b114f24accf89ac61145ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/38/f5/dc4f85c3909051823df49901e72015d2d750bd26b086480ec2\n",
            "Successfully built torchviz\n",
            "Installing collected packages: torchviz\n",
            "Successfully installed torchviz-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "C7pqy4uQlf4R",
        "outputId": "459af7a2-7539-41ea-bb31-26b666db5c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data parsing is done!\n"
          ]
        }
      ],
      "source": [
        "# Import relevant python modules\n",
        "import sys, os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# The datasets needed are computed by the `ComputeGluon.py` script in PseudoData\n",
        "filename1='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-3.dat' \n",
        "filename2='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-4.dat' \n",
        "filename3='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-5.dat' \n",
        "filename4='https://raw.githubusercontent.com/rabah-khalek/TF_tutorials/master/PseudoData/gluon_NNPDF31_nlo_pch_as_0118_xmin1e-6.dat' \n",
        "\n",
        "# Headers to skip\n",
        "lines_to_skip = 5\n",
        "\n",
        "# Defining the columns (cv = central value, sd = standard deviation)\n",
        "columns=[\"x\", \"gluon_cv\", \"gluon_sd\"]\n",
        "\n",
        "# Loading data from txt file\n",
        "# Change filename1 to another filename for data that extends to lower x \n",
        "# (see exercises at the bottom of this notebook)\n",
        "df = pd.read_csv(filename1, \n",
        "                 sep=\"\\s+\", \n",
        "                 skiprows=lines_to_skip, \n",
        "                 usecols=[0,1,2], \n",
        "                 names=columns)\n",
        "\n",
        "# Splitting data randomly to train and test using the sklearn library\n",
        "df_train, df_test = train_test_split(df,test_size=0.2,random_state=42)\n",
        "\n",
        "# Sort the split data according to their x values\n",
        "df_train = df_train.sort_values(\"x\")\n",
        "df_test = df_test.sort_values(\"x\")\n",
        "\n",
        "print(\"Data parsing is done!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torchviz import make_dot\n",
        "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data.dataset import random_split"
      ],
      "metadata": {
        "id": "_5A7yYKhpp4P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z4-H5pIDoTsD"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "##################################################################\n",
        "# Building NN from the PyTorch API (nn.Linear)\n",
        "##################################################################\n",
        "\n",
        "class Torch_Model(nn.Module):\n",
        "    \n",
        "    def __init__(self, n_features, n_neurons, n_outputs):\n",
        "\n",
        "        super(Torch_Model, self).__init__()\n",
        "        \n",
        "        self.fc1 = nn.Linear(n_features, n_neurons) # Hidden Layer 1\n",
        "        self.sig1 = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(n_neurons, n_outputs) # Output Layer\n",
        "\n",
        "    # Evaluates and returns output\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.sig1(out)\n",
        "        out = self.fc2(out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "NSwYnGISSiVc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "n_features = 1\n",
        "n_neurons = 20\n",
        "n_outputs = 1\n",
        "learning_rate = 0.001\n",
        "batch_size = 20\n",
        "n_epochs = 1000"
      ],
      "metadata": {
        "id": "7UoAUmwtS0zo"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "\n",
        "#print(df_train.to_numpy())\n",
        "#train_dataset, val_dataset = random_split(df_train.to_numpy(), [80, 20])\n",
        "\n",
        "train_loader = DataLoader(df_train.to_numpy(), batch_size=batch_size)\n",
        "val_loader = DataLoader(df_test.to_numpy(), batch_size=batch_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "d8VT9kIcTQAn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize network\n",
        "model = Torch_Model(n_features, n_neurons, n_outputs).to(device)"
      ],
      "metadata": {
        "id": "_0DAX9jfiyGJ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "PupCzpAJjMLo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    def train_step(x, y, dy):\n",
        "        model.train()\n",
        "        yhat = model(x)\n",
        "        loss = loss_fn(y/dy, yhat/dy)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item()\n",
        "    return train_step"
      ],
      "metadata": {
        "id": "wwN1dgYXuq8c"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = 1\n",
        "hidden_dim = 20\n",
        "output_dim = 1\n",
        "model = Torch_Model(input_dim, hidden_dim, output_dim)\n",
        "\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "n_epochs = 1000\n",
        "training_losses = []\n",
        "validation_losses = []\n",
        "#print(model.state_dict())\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    batch_losses = []\n",
        "    for data_batch in train_loader:\n",
        "        data_batch = data_batch.to(device)\n",
        "        x_batch = torch.t(data_batch[:,0][None]).float()\n",
        "        y_batch = torch.t(data_batch[:,1][None]).float()\n",
        "        dy_batch = torch.t(data_batch[:,2][None]).float()\n",
        "\n",
        "        loss = train_step(x_batch, y_batch, dy_batch)\n",
        "        batch_losses.append(loss)\n",
        "    training_loss = np.mean(batch_losses)\n",
        "    training_losses.append(training_loss)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        val_losses = []\n",
        "        for data_batch_val in val_loader:\n",
        "            x_batch_val = torch.t(data_batch_val[:,0][None]).float()\n",
        "            y_batch_val = torch.t(data_batch_val[:,1][None]).float()\n",
        "            dy_batch_val = torch.t(data_batch_val[:,2][None]).float()\n",
        "            model.eval()\n",
        "            yhat = model(x_batch_val)\n",
        "            val_loss = loss_fn(y_batch_val/dy_batch_val, yhat/dy_batch_val).item()\n",
        "            val_losses.append(val_loss)\n",
        "        validation_loss = np.mean(val_losses)\n",
        "        validation_losses.append(validation_loss)\n",
        "\n",
        "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
        "\n",
        "#print(model.state_dict())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HxwwZlRuRB5",
        "outputId": "1cee92d3-2491-415a-c652-2172c46dc6b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] Training loss: 529.891\t Validation loss: 491.048\n",
            "[2] Training loss: 418.818\t Validation loss: 449.511\n",
            "[3] Training loss: 405.371\t Validation loss: 436.516\n",
            "[4] Training loss: 402.509\t Validation loss: 432.867\n",
            "[5] Training loss: 401.681\t Validation loss: 431.148\n",
            "[6] Training loss: 400.815\t Validation loss: 429.584\n",
            "[7] Training loss: 399.689\t Validation loss: 427.933\n",
            "[8] Training loss: 398.377\t Validation loss: 426.199\n",
            "[9] Training loss: 396.939\t Validation loss: 424.399\n",
            "[10] Training loss: 395.409\t Validation loss: 422.543\n",
            "[11] Training loss: 393.802\t Validation loss: 420.639\n",
            "[12] Training loss: 392.130\t Validation loss: 418.694\n",
            "[13] Training loss: 390.403\t Validation loss: 416.714\n",
            "[14] Training loss: 388.629\t Validation loss: 414.704\n",
            "[15] Training loss: 386.813\t Validation loss: 412.666\n",
            "[16] Training loss: 384.961\t Validation loss: 410.605\n",
            "[17] Training loss: 383.077\t Validation loss: 408.523\n",
            "[18] Training loss: 381.166\t Validation loss: 406.422\n",
            "[19] Training loss: 379.230\t Validation loss: 404.306\n",
            "[20] Training loss: 377.273\t Validation loss: 402.174\n",
            "[21] Training loss: 375.296\t Validation loss: 400.029\n",
            "[22] Training loss: 373.302\t Validation loss: 397.871\n",
            "[23] Training loss: 371.293\t Validation loss: 395.702\n",
            "[24] Training loss: 369.269\t Validation loss: 393.522\n",
            "[25] Training loss: 367.231\t Validation loss: 391.332\n",
            "[26] Training loss: 365.182\t Validation loss: 389.132\n",
            "[27] Training loss: 363.122\t Validation loss: 386.922\n",
            "[28] Training loss: 361.050\t Validation loss: 384.704\n",
            "[29] Training loss: 358.969\t Validation loss: 382.476\n",
            "[30] Training loss: 356.878\t Validation loss: 380.239\n",
            "[31] Training loss: 354.778\t Validation loss: 377.994\n",
            "[32] Training loss: 352.670\t Validation loss: 375.741\n",
            "[33] Training loss: 350.554\t Validation loss: 373.479\n",
            "[34] Training loss: 348.430\t Validation loss: 371.210\n",
            "[35] Training loss: 346.299\t Validation loss: 368.933\n",
            "[36] Training loss: 344.161\t Validation loss: 366.648\n",
            "[37] Training loss: 342.017\t Validation loss: 364.357\n",
            "[38] Training loss: 339.868\t Validation loss: 362.059\n",
            "[39] Training loss: 337.713\t Validation loss: 359.754\n",
            "[40] Training loss: 335.554\t Validation loss: 357.442\n",
            "[41] Training loss: 333.390\t Validation loss: 355.125\n",
            "[42] Training loss: 331.223\t Validation loss: 352.803\n",
            "[43] Training loss: 329.052\t Validation loss: 350.475\n",
            "[44] Training loss: 326.879\t Validation loss: 348.142\n",
            "[45] Training loss: 324.705\t Validation loss: 345.805\n",
            "[46] Training loss: 322.528\t Validation loss: 343.465\n",
            "[47] Training loss: 320.351\t Validation loss: 341.121\n",
            "[48] Training loss: 318.174\t Validation loss: 338.774\n",
            "[49] Training loss: 315.998\t Validation loss: 336.424\n",
            "[50] Training loss: 313.823\t Validation loss: 334.073\n",
            "[51] Training loss: 311.651\t Validation loss: 331.721\n",
            "[52] Training loss: 309.482\t Validation loss: 329.369\n",
            "[53] Training loss: 307.317\t Validation loss: 327.018\n",
            "[54] Training loss: 305.157\t Validation loss: 324.667\n",
            "[55] Training loss: 303.003\t Validation loss: 322.319\n",
            "[56] Training loss: 300.857\t Validation loss: 319.975\n",
            "[57] Training loss: 298.719\t Validation loss: 317.635\n",
            "[58] Training loss: 296.591\t Validation loss: 315.300\n",
            "[59] Training loss: 294.474\t Validation loss: 312.972\n",
            "[60] Training loss: 292.370\t Validation loss: 310.652\n",
            "[61] Training loss: 290.280\t Validation loss: 308.342\n",
            "[62] Training loss: 288.206\t Validation loss: 306.043\n",
            "[63] Training loss: 286.149\t Validation loss: 303.756\n",
            "[64] Training loss: 284.112\t Validation loss: 301.484\n",
            "[65] Training loss: 282.096\t Validation loss: 299.228\n",
            "[66] Training loss: 280.103\t Validation loss: 296.990\n",
            "[67] Training loss: 278.135\t Validation loss: 294.772\n",
            "[68] Training loss: 276.195\t Validation loss: 292.577\n",
            "[69] Training loss: 274.284\t Validation loss: 290.406\n",
            "[70] Training loss: 272.404\t Validation loss: 288.262\n",
            "[71] Training loss: 270.559\t Validation loss: 286.147\n",
            "[72] Training loss: 268.750\t Validation loss: 284.064\n",
            "[73] Training loss: 266.980\t Validation loss: 282.015\n",
            "[74] Training loss: 265.250\t Validation loss: 280.004\n",
            "[75] Training loss: 263.565\t Validation loss: 278.031\n",
            "[76] Training loss: 261.923\t Validation loss: 276.101\n",
            "[77] Training loss: 260.331\t Validation loss: 274.214\n",
            "[78] Training loss: 258.784\t Validation loss: 272.376\n",
            "[79] Training loss: 257.294\t Validation loss: 270.585\n",
            "[80] Training loss: 255.848\t Validation loss: 268.851\n",
            "[81] Training loss: 254.471\t Validation loss: 267.161\n",
            "[82] Training loss: 253.122\t Validation loss: 265.543\n",
            "[83] Training loss: 251.876\t Validation loss: 263.954\n",
            "[84] Training loss: 250.601\t Validation loss: 262.474\n",
            "[85] Training loss: 249.543\t Validation loss: 260.964\n",
            "[86] Training loss: 248.268\t Validation loss: 259.689\n",
            "[87] Training loss: 247.650\t Validation loss: 258.191\n",
            "[88] Training loss: 246.286\t Validation loss: 257.432\n",
            "[89] Training loss: 247.717\t Validation loss: 256.008\n",
            "[90] Training loss: 248.379\t Validation loss: 258.111\n",
            "[91] Training loss: 262.945\t Validation loss: 260.558\n",
            "[92] Training loss: 277.607\t Validation loss: 278.642\n",
            "[93] Training loss: 291.593\t Validation loss: 278.580\n",
            "[94] Training loss: 265.738\t Validation loss: 274.872\n",
            "[95] Training loss: 257.993\t Validation loss: 259.954\n",
            "[96] Training loss: 240.659\t Validation loss: 255.081\n",
            "[97] Training loss: 241.914\t Validation loss: 251.067\n",
            "[98] Training loss: 236.500\t Validation loss: 250.099\n",
            "[99] Training loss: 237.891\t Validation loss: 248.825\n",
            "[100] Training loss: 235.765\t Validation loss: 248.410\n",
            "[101] Training loss: 236.430\t Validation loss: 247.684\n",
            "[102] Training loss: 235.330\t Validation loss: 247.308\n",
            "[103] Training loss: 235.589\t Validation loss: 246.740\n",
            "[104] Training loss: 234.888\t Validation loss: 246.357\n",
            "[105] Training loss: 234.952\t Validation loss: 245.856\n",
            "[106] Training loss: 234.437\t Validation loss: 245.473\n",
            "[107] Training loss: 234.408\t Validation loss: 245.013\n",
            "[108] Training loss: 233.993\t Validation loss: 244.636\n",
            "[109] Training loss: 233.920\t Validation loss: 244.205\n",
            "[110] Training loss: 233.563\t Validation loss: 243.840\n",
            "[111] Training loss: 233.471\t Validation loss: 243.433\n",
            "[112] Training loss: 233.151\t Validation loss: 243.082\n",
            "[113] Training loss: 233.054\t Validation loss: 242.697\n",
            "[114] Training loss: 232.760\t Validation loss: 242.362\n",
            "[115] Training loss: 232.665\t Validation loss: 241.996\n",
            "[116] Training loss: 232.387\t Validation loss: 241.678\n",
            "[117] Training loss: 232.301\t Validation loss: 241.330\n",
            "[118] Training loss: 232.033\t Validation loss: 241.029\n",
            "[119] Training loss: 231.959\t Validation loss: 240.696\n",
            "[120] Training loss: 231.695\t Validation loss: 240.413\n",
            "[121] Training loss: 231.637\t Validation loss: 240.095\n",
            "[122] Training loss: 231.371\t Validation loss: 239.830\n",
            "[123] Training loss: 231.333\t Validation loss: 239.525\n",
            "[124] Training loss: 231.060\t Validation loss: 239.278\n",
            "[125] Training loss: 231.048\t Validation loss: 238.983\n",
            "[126] Training loss: 230.760\t Validation loss: 238.756\n",
            "[127] Training loss: 230.779\t Validation loss: 238.469\n",
            "[128] Training loss: 230.468\t Validation loss: 238.262\n",
            "[129] Training loss: 230.527\t Validation loss: 237.981\n",
            "[130] Training loss: 230.182\t Validation loss: 237.795\n",
            "[131] Training loss: 230.293\t Validation loss: 237.517\n",
            "[132] Training loss: 229.902\t Validation loss: 237.354\n",
            "[133] Training loss: 230.078\t Validation loss: 237.077\n",
            "[134] Training loss: 229.626\t Validation loss: 236.941\n",
            "[135] Training loss: 229.887\t Validation loss: 236.659\n",
            "[136] Training loss: 229.353\t Validation loss: 236.554\n",
            "[137] Training loss: 229.722\t Validation loss: 236.264\n",
            "[138] Training loss: 229.086\t Validation loss: 236.195\n",
            "[139] Training loss: 229.592\t Validation loss: 235.894\n",
            "[140] Training loss: 228.827\t Validation loss: 235.868\n",
            "[141] Training loss: 229.505\t Validation loss: 235.550\n",
            "[142] Training loss: 228.583\t Validation loss: 235.577\n",
            "[143] Training loss: 229.471\t Validation loss: 235.239\n",
            "[144] Training loss: 228.360\t Validation loss: 235.327\n",
            "[145] Training loss: 229.497\t Validation loss: 234.964\n",
            "[146] Training loss: 228.169\t Validation loss: 235.123\n",
            "[147] Training loss: 229.587\t Validation loss: 234.731\n",
            "[148] Training loss: 228.015\t Validation loss: 234.965\n",
            "[149] Training loss: 229.726\t Validation loss: 234.537\n",
            "[150] Training loss: 227.892\t Validation loss: 234.843\n",
            "[151] Training loss: 229.879\t Validation loss: 234.373\n",
            "[152] Training loss: 227.780\t Validation loss: 234.731\n",
            "[153] Training loss: 229.982\t Validation loss: 234.213\n",
            "[154] Training loss: 227.641\t Validation loss: 234.592\n",
            "[155] Training loss: 229.964\t Validation loss: 234.023\n",
            "[156] Training loss: 227.437\t Validation loss: 234.386\n",
            "[157] Training loss: 229.773\t Validation loss: 233.779\n",
            "[158] Training loss: 227.158\t Validation loss: 234.096\n",
            "[159] Training loss: 229.411\t Validation loss: 233.473\n",
            "[160] Training loss: 226.822\t Validation loss: 233.731\n",
            "[161] Training loss: 228.924\t Validation loss: 233.124\n",
            "[162] Training loss: 226.471\t Validation loss: 233.325\n",
            "[163] Training loss: 228.380\t Validation loss: 232.758\n",
            "[164] Training loss: 226.141\t Validation loss: 232.911\n",
            "[165] Training loss: 227.839\t Validation loss: 232.400\n",
            "[166] Training loss: 225.851\t Validation loss: 232.515\n",
            "[167] Training loss: 227.337\t Validation loss: 232.063\n",
            "[168] Training loss: 225.606\t Validation loss: 232.148\n",
            "[169] Training loss: 226.889\t Validation loss: 231.752\n",
            "[170] Training loss: 225.399\t Validation loss: 231.812\n",
            "[171] Training loss: 226.495\t Validation loss: 231.466\n",
            "[172] Training loss: 225.221\t Validation loss: 231.504\n",
            "[173] Training loss: 226.149\t Validation loss: 231.200\n",
            "[174] Training loss: 225.062\t Validation loss: 231.219\n",
            "[175] Training loss: 225.842\t Validation loss: 230.950\n",
            "[176] Training loss: 224.914\t Validation loss: 230.952\n",
            "[177] Training loss: 225.566\t Validation loss: 230.712\n",
            "[178] Training loss: 224.773\t Validation loss: 230.700\n",
            "[179] Training loss: 225.313\t Validation loss: 230.483\n",
            "[180] Training loss: 224.634\t Validation loss: 230.458\n",
            "[181] Training loss: 225.079\t Validation loss: 230.261\n",
            "[182] Training loss: 224.495\t Validation loss: 230.225\n",
            "[183] Training loss: 224.858\t Validation loss: 230.044\n",
            "[184] Training loss: 224.355\t Validation loss: 229.998\n",
            "[185] Training loss: 224.648\t Validation loss: 229.831\n",
            "[186] Training loss: 224.212\t Validation loss: 229.777\n",
            "[187] Training loss: 224.446\t Validation loss: 229.620\n",
            "[188] Training loss: 224.067\t Validation loss: 229.559\n",
            "[189] Training loss: 224.249\t Validation loss: 229.412\n",
            "[190] Training loss: 223.918\t Validation loss: 229.344\n",
            "[191] Training loss: 224.057\t Validation loss: 229.205\n",
            "[192] Training loss: 223.766\t Validation loss: 229.132\n",
            "[193] Training loss: 223.868\t Validation loss: 228.999\n",
            "[194] Training loss: 223.612\t Validation loss: 228.921\n",
            "[195] Training loss: 223.681\t Validation loss: 228.794\n",
            "[196] Training loss: 223.454\t Validation loss: 228.712\n",
            "[197] Training loss: 223.497\t Validation loss: 228.589\n",
            "[198] Training loss: 223.293\t Validation loss: 228.505\n",
            "[199] Training loss: 223.313\t Validation loss: 228.385\n",
            "[200] Training loss: 223.130\t Validation loss: 228.298\n",
            "[201] Training loss: 223.131\t Validation loss: 228.182\n",
            "[202] Training loss: 222.964\t Validation loss: 228.091\n",
            "[203] Training loss: 222.948\t Validation loss: 227.978\n",
            "[204] Training loss: 222.796\t Validation loss: 227.886\n",
            "[205] Training loss: 222.767\t Validation loss: 227.774\n",
            "[206] Training loss: 222.626\t Validation loss: 227.680\n",
            "[207] Training loss: 222.585\t Validation loss: 227.570\n",
            "[208] Training loss: 222.453\t Validation loss: 227.475\n",
            "[209] Training loss: 222.402\t Validation loss: 227.366\n",
            "[210] Training loss: 222.279\t Validation loss: 227.269\n",
            "[211] Training loss: 222.220\t Validation loss: 227.162\n",
            "[212] Training loss: 222.102\t Validation loss: 227.064\n",
            "[213] Training loss: 222.037\t Validation loss: 226.957\n",
            "[214] Training loss: 221.924\t Validation loss: 226.858\n",
            "[215] Training loss: 221.853\t Validation loss: 226.752\n",
            "[216] Training loss: 221.745\t Validation loss: 226.651\n",
            "[217] Training loss: 221.669\t Validation loss: 226.546\n",
            "[218] Training loss: 221.563\t Validation loss: 226.445\n",
            "[219] Training loss: 221.484\t Validation loss: 226.339\n",
            "[220] Training loss: 221.380\t Validation loss: 226.238\n",
            "[221] Training loss: 221.297\t Validation loss: 226.132\n",
            "[222] Training loss: 221.196\t Validation loss: 226.030\n",
            "[223] Training loss: 221.110\t Validation loss: 225.924\n",
            "[224] Training loss: 221.010\t Validation loss: 225.821\n",
            "[225] Training loss: 220.922\t Validation loss: 225.716\n",
            "[226] Training loss: 220.823\t Validation loss: 225.612\n",
            "[227] Training loss: 220.733\t Validation loss: 225.506\n",
            "[228] Training loss: 220.635\t Validation loss: 225.402\n",
            "[229] Training loss: 220.543\t Validation loss: 225.296\n",
            "[230] Training loss: 220.445\t Validation loss: 225.191\n",
            "[231] Training loss: 220.352\t Validation loss: 225.085\n",
            "[232] Training loss: 220.253\t Validation loss: 224.979\n",
            "[233] Training loss: 220.159\t Validation loss: 224.872\n",
            "[234] Training loss: 220.061\t Validation loss: 224.766\n",
            "[235] Training loss: 219.965\t Validation loss: 224.659\n",
            "[236] Training loss: 219.867\t Validation loss: 224.552\n",
            "[237] Training loss: 219.770\t Validation loss: 224.444\n",
            "[238] Training loss: 219.671\t Validation loss: 224.337\n",
            "[239] Training loss: 219.574\t Validation loss: 224.229\n",
            "[240] Training loss: 219.475\t Validation loss: 224.120\n",
            "[241] Training loss: 219.376\t Validation loss: 224.012\n",
            "[242] Training loss: 219.276\t Validation loss: 223.903\n",
            "[243] Training loss: 219.177\t Validation loss: 223.794\n",
            "[244] Training loss: 219.077\t Validation loss: 223.684\n",
            "[245] Training loss: 218.977\t Validation loss: 223.574\n",
            "[246] Training loss: 218.876\t Validation loss: 223.464\n",
            "[247] Training loss: 218.775\t Validation loss: 223.353\n",
            "[248] Training loss: 218.673\t Validation loss: 223.243\n",
            "[249] Training loss: 218.572\t Validation loss: 223.131\n",
            "[250] Training loss: 218.469\t Validation loss: 223.020\n",
            "[251] Training loss: 218.367\t Validation loss: 222.908\n",
            "[252] Training loss: 218.264\t Validation loss: 222.795\n",
            "[253] Training loss: 218.161\t Validation loss: 222.683\n",
            "[254] Training loss: 218.057\t Validation loss: 222.570\n",
            "[255] Training loss: 217.953\t Validation loss: 222.456\n",
            "[256] Training loss: 217.849\t Validation loss: 222.342\n",
            "[257] Training loss: 217.744\t Validation loss: 222.228\n",
            "[258] Training loss: 217.639\t Validation loss: 222.113\n",
            "[259] Training loss: 217.533\t Validation loss: 221.998\n",
            "[260] Training loss: 217.427\t Validation loss: 221.883\n",
            "[261] Training loss: 217.321\t Validation loss: 221.767\n",
            "[262] Training loss: 217.214\t Validation loss: 221.651\n",
            "[263] Training loss: 217.106\t Validation loss: 221.534\n",
            "[264] Training loss: 216.999\t Validation loss: 221.417\n",
            "[265] Training loss: 216.891\t Validation loss: 221.300\n",
            "[266] Training loss: 216.782\t Validation loss: 221.182\n",
            "[267] Training loss: 216.673\t Validation loss: 221.064\n",
            "[268] Training loss: 216.564\t Validation loss: 220.945\n",
            "[269] Training loss: 216.454\t Validation loss: 220.825\n",
            "[270] Training loss: 216.344\t Validation loss: 220.706\n",
            "[271] Training loss: 216.234\t Validation loss: 220.586\n",
            "[272] Training loss: 216.122\t Validation loss: 220.465\n",
            "[273] Training loss: 216.011\t Validation loss: 220.344\n",
            "[274] Training loss: 215.899\t Validation loss: 220.222\n",
            "[275] Training loss: 215.787\t Validation loss: 220.101\n",
            "[276] Training loss: 215.674\t Validation loss: 219.978\n",
            "[277] Training loss: 215.561\t Validation loss: 219.855\n",
            "[278] Training loss: 215.447\t Validation loss: 219.732\n",
            "[279] Training loss: 215.333\t Validation loss: 219.608\n",
            "[280] Training loss: 215.218\t Validation loss: 219.483\n",
            "[281] Training loss: 215.103\t Validation loss: 219.359\n",
            "[282] Training loss: 214.987\t Validation loss: 219.233\n",
            "[283] Training loss: 214.871\t Validation loss: 219.107\n",
            "[284] Training loss: 214.754\t Validation loss: 218.981\n",
            "[285] Training loss: 214.637\t Validation loss: 218.854\n",
            "[286] Training loss: 214.520\t Validation loss: 218.727\n",
            "[287] Training loss: 214.402\t Validation loss: 218.599\n",
            "[288] Training loss: 214.283\t Validation loss: 218.470\n",
            "[289] Training loss: 214.164\t Validation loss: 218.341\n",
            "[290] Training loss: 214.045\t Validation loss: 218.212\n",
            "[291] Training loss: 213.925\t Validation loss: 218.082\n",
            "[292] Training loss: 213.804\t Validation loss: 217.951\n",
            "[293] Training loss: 213.683\t Validation loss: 217.820\n",
            "[294] Training loss: 213.561\t Validation loss: 217.688\n",
            "[295] Training loss: 213.439\t Validation loss: 217.556\n",
            "[296] Training loss: 213.317\t Validation loss: 217.424\n",
            "[297] Training loss: 213.194\t Validation loss: 217.290\n",
            "[298] Training loss: 213.070\t Validation loss: 217.156\n",
            "[299] Training loss: 212.946\t Validation loss: 217.022\n",
            "[300] Training loss: 212.821\t Validation loss: 216.887\n",
            "[301] Training loss: 212.696\t Validation loss: 216.751\n",
            "[302] Training loss: 212.570\t Validation loss: 216.615\n",
            "[303] Training loss: 212.444\t Validation loss: 216.478\n",
            "[304] Training loss: 212.317\t Validation loss: 216.341\n",
            "[305] Training loss: 212.189\t Validation loss: 216.203\n",
            "[306] Training loss: 212.061\t Validation loss: 216.065\n",
            "[307] Training loss: 211.933\t Validation loss: 215.926\n",
            "[308] Training loss: 211.804\t Validation loss: 215.786\n",
            "[309] Training loss: 211.674\t Validation loss: 215.646\n",
            "[310] Training loss: 211.544\t Validation loss: 215.505\n",
            "[311] Training loss: 211.413\t Validation loss: 215.363\n",
            "[312] Training loss: 211.282\t Validation loss: 215.221\n",
            "[313] Training loss: 211.150\t Validation loss: 215.078\n",
            "[314] Training loss: 211.017\t Validation loss: 214.935\n",
            "[315] Training loss: 210.884\t Validation loss: 214.791\n",
            "[316] Training loss: 210.750\t Validation loss: 214.646\n",
            "[317] Training loss: 210.616\t Validation loss: 214.501\n",
            "[318] Training loss: 210.481\t Validation loss: 214.355\n",
            "[319] Training loss: 210.345\t Validation loss: 214.208\n",
            "[320] Training loss: 210.209\t Validation loss: 214.061\n",
            "[321] Training loss: 210.072\t Validation loss: 213.913\n",
            "[322] Training loss: 209.935\t Validation loss: 213.765\n",
            "[323] Training loss: 209.797\t Validation loss: 213.616\n",
            "[324] Training loss: 209.659\t Validation loss: 213.466\n",
            "[325] Training loss: 209.519\t Validation loss: 213.315\n",
            "[326] Training loss: 209.379\t Validation loss: 213.164\n",
            "[327] Training loss: 209.239\t Validation loss: 213.012\n",
            "[328] Training loss: 209.098\t Validation loss: 212.859\n",
            "[329] Training loss: 208.956\t Validation loss: 212.706\n",
            "[330] Training loss: 208.813\t Validation loss: 212.552\n",
            "[331] Training loss: 208.670\t Validation loss: 212.398\n",
            "[332] Training loss: 208.527\t Validation loss: 212.242\n",
            "[333] Training loss: 208.382\t Validation loss: 212.086\n",
            "[334] Training loss: 208.237\t Validation loss: 211.929\n",
            "[335] Training loss: 208.091\t Validation loss: 211.772\n",
            "[336] Training loss: 207.945\t Validation loss: 211.614\n",
            "[337] Training loss: 207.798\t Validation loss: 211.455\n",
            "[338] Training loss: 207.650\t Validation loss: 211.295\n",
            "[339] Training loss: 207.501\t Validation loss: 211.135\n",
            "[340] Training loss: 207.352\t Validation loss: 210.973\n",
            "[341] Training loss: 207.202\t Validation loss: 210.811\n",
            "[342] Training loss: 207.051\t Validation loss: 210.649\n",
            "[343] Training loss: 206.900\t Validation loss: 210.485\n",
            "[344] Training loss: 206.748\t Validation loss: 210.321\n",
            "[345] Training loss: 206.595\t Validation loss: 210.156\n",
            "[346] Training loss: 206.442\t Validation loss: 209.991\n",
            "[347] Training loss: 206.288\t Validation loss: 209.824\n",
            "[348] Training loss: 206.133\t Validation loss: 209.657\n",
            "[349] Training loss: 205.977\t Validation loss: 209.489\n",
            "[350] Training loss: 205.821\t Validation loss: 209.320\n",
            "[351] Training loss: 205.663\t Validation loss: 209.150\n",
            "[352] Training loss: 205.506\t Validation loss: 208.980\n",
            "[353] Training loss: 205.347\t Validation loss: 208.809\n",
            "[354] Training loss: 205.187\t Validation loss: 208.637\n",
            "[355] Training loss: 205.027\t Validation loss: 208.464\n",
            "[356] Training loss: 204.866\t Validation loss: 208.290\n",
            "[357] Training loss: 204.705\t Validation loss: 208.116\n",
            "[358] Training loss: 204.542\t Validation loss: 207.940\n",
            "[359] Training loss: 204.379\t Validation loss: 207.764\n",
            "[360] Training loss: 204.215\t Validation loss: 207.587\n",
            "[361] Training loss: 204.050\t Validation loss: 207.410\n",
            "[362] Training loss: 203.884\t Validation loss: 207.231\n",
            "[363] Training loss: 203.718\t Validation loss: 207.051\n",
            "[364] Training loss: 203.550\t Validation loss: 206.871\n",
            "[365] Training loss: 203.382\t Validation loss: 206.690\n",
            "[366] Training loss: 203.213\t Validation loss: 206.508\n",
            "[367] Training loss: 203.044\t Validation loss: 206.325\n",
            "[368] Training loss: 202.873\t Validation loss: 206.141\n",
            "[369] Training loss: 202.702\t Validation loss: 205.956\n",
            "[370] Training loss: 202.529\t Validation loss: 205.770\n",
            "[371] Training loss: 202.356\t Validation loss: 205.584\n",
            "[372] Training loss: 202.182\t Validation loss: 205.396\n",
            "[373] Training loss: 202.008\t Validation loss: 205.208\n",
            "[374] Training loss: 201.832\t Validation loss: 205.019\n",
            "[375] Training loss: 201.656\t Validation loss: 204.829\n",
            "[376] Training loss: 201.478\t Validation loss: 204.638\n",
            "[377] Training loss: 201.300\t Validation loss: 204.446\n",
            "[378] Training loss: 201.121\t Validation loss: 204.253\n",
            "[379] Training loss: 200.941\t Validation loss: 204.059\n",
            "[380] Training loss: 200.760\t Validation loss: 203.864\n",
            "[381] Training loss: 200.578\t Validation loss: 203.668\n",
            "[382] Training loss: 200.395\t Validation loss: 203.472\n",
            "[383] Training loss: 200.212\t Validation loss: 203.274\n",
            "[384] Training loss: 200.027\t Validation loss: 203.075\n",
            "[385] Training loss: 199.842\t Validation loss: 202.876\n",
            "[386] Training loss: 199.656\t Validation loss: 202.675\n",
            "[387] Training loss: 199.468\t Validation loss: 202.474\n",
            "[388] Training loss: 199.280\t Validation loss: 202.271\n",
            "[389] Training loss: 199.091\t Validation loss: 202.067\n",
            "[390] Training loss: 198.901\t Validation loss: 201.863\n",
            "[391] Training loss: 198.710\t Validation loss: 201.657\n",
            "[392] Training loss: 198.518\t Validation loss: 201.451\n",
            "[393] Training loss: 198.325\t Validation loss: 201.243\n",
            "[394] Training loss: 198.131\t Validation loss: 201.035\n",
            "[395] Training loss: 197.936\t Validation loss: 200.825\n",
            "[396] Training loss: 197.741\t Validation loss: 200.614\n",
            "[397] Training loss: 197.544\t Validation loss: 200.403\n",
            "[398] Training loss: 197.346\t Validation loss: 200.190\n",
            "[399] Training loss: 197.147\t Validation loss: 199.976\n",
            "[400] Training loss: 196.947\t Validation loss: 199.761\n",
            "[401] Training loss: 196.747\t Validation loss: 199.545\n",
            "[402] Training loss: 196.545\t Validation loss: 199.328\n",
            "[403] Training loss: 196.342\t Validation loss: 199.110\n",
            "[404] Training loss: 196.138\t Validation loss: 198.891\n",
            "[405] Training loss: 195.933\t Validation loss: 198.671\n",
            "[406] Training loss: 195.727\t Validation loss: 198.450\n",
            "[407] Training loss: 195.521\t Validation loss: 198.227\n",
            "[408] Training loss: 195.313\t Validation loss: 198.004\n",
            "[409] Training loss: 195.104\t Validation loss: 197.779\n",
            "[410] Training loss: 194.894\t Validation loss: 197.554\n",
            "[411] Training loss: 194.682\t Validation loss: 197.327\n",
            "[412] Training loss: 194.470\t Validation loss: 197.099\n",
            "[413] Training loss: 194.257\t Validation loss: 196.870\n",
            "[414] Training loss: 194.043\t Validation loss: 196.640\n",
            "[415] Training loss: 193.827\t Validation loss: 196.408\n",
            "[416] Training loss: 193.611\t Validation loss: 196.176\n",
            "[417] Training loss: 193.393\t Validation loss: 195.942\n",
            "[418] Training loss: 193.174\t Validation loss: 195.707\n",
            "[419] Training loss: 192.954\t Validation loss: 195.471\n",
            "[420] Training loss: 192.734\t Validation loss: 195.234\n",
            "[421] Training loss: 192.511\t Validation loss: 194.996\n",
            "[422] Training loss: 192.288\t Validation loss: 194.756\n",
            "[423] Training loss: 192.064\t Validation loss: 194.516\n",
            "[424] Training loss: 191.839\t Validation loss: 194.274\n",
            "[425] Training loss: 191.612\t Validation loss: 194.031\n",
            "[426] Training loss: 191.384\t Validation loss: 193.786\n",
            "[427] Training loss: 191.155\t Validation loss: 193.541\n",
            "[428] Training loss: 190.925\t Validation loss: 193.294\n",
            "[429] Training loss: 190.694\t Validation loss: 193.046\n",
            "[430] Training loss: 190.462\t Validation loss: 192.797\n",
            "[431] Training loss: 190.228\t Validation loss: 192.546\n",
            "[432] Training loss: 189.993\t Validation loss: 192.294\n",
            "[433] Training loss: 189.757\t Validation loss: 192.041\n",
            "[434] Training loss: 189.520\t Validation loss: 191.787\n",
            "[435] Training loss: 189.282\t Validation loss: 191.532\n",
            "[436] Training loss: 189.042\t Validation loss: 191.275\n",
            "[437] Training loss: 188.801\t Validation loss: 191.017\n",
            "[438] Training loss: 188.559\t Validation loss: 190.757\n",
            "[439] Training loss: 188.316\t Validation loss: 190.497\n",
            "[440] Training loss: 188.072\t Validation loss: 190.235\n",
            "[441] Training loss: 187.826\t Validation loss: 189.972\n",
            "[442] Training loss: 187.579\t Validation loss: 189.707\n",
            "[443] Training loss: 187.331\t Validation loss: 189.441\n",
            "[444] Training loss: 187.081\t Validation loss: 189.174\n",
            "[445] Training loss: 186.830\t Validation loss: 188.905\n",
            "[446] Training loss: 186.578\t Validation loss: 188.635\n",
            "[447] Training loss: 186.325\t Validation loss: 188.364\n",
            "[448] Training loss: 186.070\t Validation loss: 188.092\n",
            "[449] Training loss: 185.814\t Validation loss: 187.818\n",
            "[450] Training loss: 185.557\t Validation loss: 187.542\n",
            "[451] Training loss: 185.298\t Validation loss: 187.266\n",
            "[452] Training loss: 185.038\t Validation loss: 186.988\n",
            "[453] Training loss: 184.777\t Validation loss: 186.708\n",
            "[454] Training loss: 184.514\t Validation loss: 186.427\n",
            "[455] Training loss: 184.250\t Validation loss: 186.145\n",
            "[456] Training loss: 183.985\t Validation loss: 185.861\n",
            "[457] Training loss: 183.718\t Validation loss: 185.576\n",
            "[458] Training loss: 183.450\t Validation loss: 185.290\n",
            "[459] Training loss: 183.181\t Validation loss: 185.002\n",
            "[460] Training loss: 182.910\t Validation loss: 184.712\n",
            "[461] Training loss: 182.638\t Validation loss: 184.421\n",
            "[462] Training loss: 182.364\t Validation loss: 184.129\n",
            "[463] Training loss: 182.089\t Validation loss: 183.836\n",
            "[464] Training loss: 181.812\t Validation loss: 183.540\n",
            "[465] Training loss: 181.535\t Validation loss: 183.244\n",
            "[466] Training loss: 181.255\t Validation loss: 182.946\n",
            "[467] Training loss: 180.975\t Validation loss: 182.646\n",
            "[468] Training loss: 180.693\t Validation loss: 182.345\n",
            "[469] Training loss: 180.409\t Validation loss: 182.042\n",
            "[470] Training loss: 180.124\t Validation loss: 181.738\n",
            "[471] Training loss: 179.838\t Validation loss: 181.433\n",
            "[472] Training loss: 179.550\t Validation loss: 181.125\n",
            "[473] Training loss: 179.261\t Validation loss: 180.817\n",
            "[474] Training loss: 178.970\t Validation loss: 180.507\n",
            "[475] Training loss: 178.677\t Validation loss: 180.195\n",
            "[476] Training loss: 178.384\t Validation loss: 179.882\n",
            "[477] Training loss: 178.088\t Validation loss: 179.567\n",
            "[478] Training loss: 177.792\t Validation loss: 179.251\n",
            "[479] Training loss: 177.493\t Validation loss: 178.933\n",
            "[480] Training loss: 177.194\t Validation loss: 178.613\n",
            "[481] Training loss: 176.892\t Validation loss: 178.292\n",
            "[482] Training loss: 176.590\t Validation loss: 177.970\n",
            "[483] Training loss: 176.285\t Validation loss: 177.645\n",
            "[484] Training loss: 175.979\t Validation loss: 177.320\n",
            "[485] Training loss: 175.672\t Validation loss: 176.992\n",
            "[486] Training loss: 175.363\t Validation loss: 176.663\n",
            "[487] Training loss: 175.052\t Validation loss: 176.333\n",
            "[488] Training loss: 174.740\t Validation loss: 176.000\n",
            "[489] Training loss: 174.427\t Validation loss: 175.666\n",
            "[490] Training loss: 174.112\t Validation loss: 175.331\n",
            "[491] Training loss: 173.795\t Validation loss: 174.994\n",
            "[492] Training loss: 173.476\t Validation loss: 174.655\n",
            "[493] Training loss: 173.157\t Validation loss: 174.315\n",
            "[494] Training loss: 172.835\t Validation loss: 173.973\n",
            "[495] Training loss: 172.512\t Validation loss: 173.629\n",
            "[496] Training loss: 172.187\t Validation loss: 173.284\n",
            "[497] Training loss: 171.861\t Validation loss: 172.937\n",
            "[498] Training loss: 171.533\t Validation loss: 172.588\n",
            "[499] Training loss: 171.203\t Validation loss: 172.238\n",
            "[500] Training loss: 170.872\t Validation loss: 171.886\n",
            "[501] Training loss: 170.539\t Validation loss: 171.532\n",
            "[502] Training loss: 170.204\t Validation loss: 171.177\n",
            "[503] Training loss: 169.868\t Validation loss: 170.819\n",
            "[504] Training loss: 169.531\t Validation loss: 170.461\n",
            "[505] Training loss: 169.191\t Validation loss: 170.100\n",
            "[506] Training loss: 168.850\t Validation loss: 169.738\n",
            "[507] Training loss: 168.507\t Validation loss: 169.374\n",
            "[508] Training loss: 168.163\t Validation loss: 169.009\n",
            "[509] Training loss: 167.817\t Validation loss: 168.641\n",
            "[510] Training loss: 167.469\t Validation loss: 168.272\n",
            "[511] Training loss: 167.119\t Validation loss: 167.902\n",
            "[512] Training loss: 166.768\t Validation loss: 167.529\n",
            "[513] Training loss: 166.416\t Validation loss: 167.155\n",
            "[514] Training loss: 166.061\t Validation loss: 166.779\n",
            "[515] Training loss: 165.705\t Validation loss: 166.401\n",
            "[516] Training loss: 165.347\t Validation loss: 166.022\n",
            "[517] Training loss: 164.987\t Validation loss: 165.641\n",
            "[518] Training loss: 164.626\t Validation loss: 165.258\n",
            "[519] Training loss: 164.263\t Validation loss: 164.874\n",
            "[520] Training loss: 163.898\t Validation loss: 164.487\n",
            "[521] Training loss: 163.532\t Validation loss: 164.099\n",
            "[522] Training loss: 163.164\t Validation loss: 163.709\n",
            "[523] Training loss: 162.794\t Validation loss: 163.318\n",
            "[524] Training loss: 162.423\t Validation loss: 162.925\n",
            "[525] Training loss: 162.049\t Validation loss: 162.530\n",
            "[526] Training loss: 161.674\t Validation loss: 162.133\n",
            "[527] Training loss: 161.298\t Validation loss: 161.734\n",
            "[528] Training loss: 160.919\t Validation loss: 161.334\n",
            "[529] Training loss: 160.539\t Validation loss: 160.932\n",
            "[530] Training loss: 160.158\t Validation loss: 160.528\n",
            "[531] Training loss: 159.774\t Validation loss: 160.123\n",
            "[532] Training loss: 159.389\t Validation loss: 159.716\n",
            "[533] Training loss: 159.002\t Validation loss: 159.307\n",
            "[534] Training loss: 158.613\t Validation loss: 158.896\n",
            "[535] Training loss: 158.223\t Validation loss: 158.483\n",
            "[536] Training loss: 157.831\t Validation loss: 158.069\n",
            "[537] Training loss: 157.437\t Validation loss: 157.653\n",
            "[538] Training loss: 157.041\t Validation loss: 157.236\n",
            "[539] Training loss: 156.644\t Validation loss: 156.816\n",
            "[540] Training loss: 156.245\t Validation loss: 156.395\n",
            "[541] Training loss: 155.844\t Validation loss: 155.972\n",
            "[542] Training loss: 155.442\t Validation loss: 155.547\n",
            "[543] Training loss: 155.038\t Validation loss: 155.121\n",
            "[544] Training loss: 154.632\t Validation loss: 154.693\n",
            "[545] Training loss: 154.225\t Validation loss: 154.263\n",
            "[546] Training loss: 153.815\t Validation loss: 153.832\n",
            "[547] Training loss: 153.404\t Validation loss: 153.398\n",
            "[548] Training loss: 152.992\t Validation loss: 152.964\n",
            "[549] Training loss: 152.578\t Validation loss: 152.527\n",
            "[550] Training loss: 152.162\t Validation loss: 152.089\n",
            "[551] Training loss: 151.744\t Validation loss: 151.648\n",
            "[552] Training loss: 151.325\t Validation loss: 151.207\n",
            "[553] Training loss: 150.903\t Validation loss: 150.763\n",
            "[554] Training loss: 150.481\t Validation loss: 150.318\n",
            "[555] Training loss: 150.056\t Validation loss: 149.871\n",
            "[556] Training loss: 149.630\t Validation loss: 149.423\n",
            "[557] Training loss: 149.203\t Validation loss: 148.973\n",
            "[558] Training loss: 148.773\t Validation loss: 148.521\n",
            "[559] Training loss: 148.342\t Validation loss: 148.067\n",
            "[560] Training loss: 147.910\t Validation loss: 147.612\n",
            "[561] Training loss: 147.475\t Validation loss: 147.156\n",
            "[562] Training loss: 147.039\t Validation loss: 146.697\n",
            "[563] Training loss: 146.602\t Validation loss: 146.237\n",
            "[564] Training loss: 146.162\t Validation loss: 145.775\n",
            "[565] Training loss: 145.722\t Validation loss: 145.312\n",
            "[566] Training loss: 145.279\t Validation loss: 144.847\n",
            "[567] Training loss: 144.835\t Validation loss: 144.381\n",
            "[568] Training loss: 144.389\t Validation loss: 143.913\n",
            "[569] Training loss: 143.942\t Validation loss: 143.443\n",
            "[570] Training loss: 143.493\t Validation loss: 142.972\n",
            "[571] Training loss: 143.043\t Validation loss: 142.499\n",
            "[572] Training loss: 142.591\t Validation loss: 142.025\n",
            "[573] Training loss: 142.137\t Validation loss: 141.549\n",
            "[574] Training loss: 141.682\t Validation loss: 141.071\n",
            "[575] Training loss: 141.225\t Validation loss: 140.592\n",
            "[576] Training loss: 140.767\t Validation loss: 140.112\n",
            "[577] Training loss: 140.307\t Validation loss: 139.630\n",
            "[578] Training loss: 139.845\t Validation loss: 139.146\n",
            "[579] Training loss: 139.382\t Validation loss: 138.661\n",
            "[580] Training loss: 138.918\t Validation loss: 138.174\n",
            "[581] Training loss: 138.452\t Validation loss: 137.687\n",
            "[582] Training loss: 137.984\t Validation loss: 137.197\n",
            "[583] Training loss: 137.515\t Validation loss: 136.706\n",
            "[584] Training loss: 137.045\t Validation loss: 136.214\n",
            "[585] Training loss: 136.573\t Validation loss: 135.720\n",
            "[586] Training loss: 136.099\t Validation loss: 135.225\n",
            "[587] Training loss: 135.624\t Validation loss: 134.729\n",
            "[588] Training loss: 135.148\t Validation loss: 134.231\n",
            "[589] Training loss: 134.670\t Validation loss: 133.731\n",
            "[590] Training loss: 134.191\t Validation loss: 133.231\n",
            "[591] Training loss: 133.710\t Validation loss: 132.729\n",
            "[592] Training loss: 133.228\t Validation loss: 132.226\n",
            "[593] Training loss: 132.745\t Validation loss: 131.721\n",
            "[594] Training loss: 132.260\t Validation loss: 131.215\n",
            "[595] Training loss: 131.774\t Validation loss: 130.708\n",
            "[596] Training loss: 131.286\t Validation loss: 130.199\n",
            "[597] Training loss: 130.797\t Validation loss: 129.690\n",
            "[598] Training loss: 130.307\t Validation loss: 129.179\n",
            "[599] Training loss: 129.816\t Validation loss: 128.667\n",
            "[600] Training loss: 129.323\t Validation loss: 128.154\n",
            "[601] Training loss: 128.829\t Validation loss: 127.639\n",
            "[602] Training loss: 128.334\t Validation loss: 127.124\n",
            "[603] Training loss: 127.838\t Validation loss: 126.607\n",
            "[604] Training loss: 127.340\t Validation loss: 126.089\n",
            "[605] Training loss: 126.841\t Validation loss: 125.570\n",
            "[606] Training loss: 126.341\t Validation loss: 125.050\n",
            "[607] Training loss: 125.840\t Validation loss: 124.529\n",
            "[608] Training loss: 125.338\t Validation loss: 124.007\n",
            "[609] Training loss: 124.835\t Validation loss: 123.484\n",
            "[610] Training loss: 124.330\t Validation loss: 122.960\n",
            "[611] Training loss: 123.825\t Validation loss: 122.434\n",
            "[612] Training loss: 123.318\t Validation loss: 121.908\n",
            "[613] Training loss: 122.810\t Validation loss: 121.381\n",
            "[614] Training loss: 122.302\t Validation loss: 120.854\n",
            "[615] Training loss: 121.792\t Validation loss: 120.325\n",
            "[616] Training loss: 121.282\t Validation loss: 119.795\n",
            "[617] Training loss: 120.770\t Validation loss: 119.265\n",
            "[618] Training loss: 120.258\t Validation loss: 118.733\n",
            "[619] Training loss: 119.744\t Validation loss: 118.201\n",
            "[620] Training loss: 119.230\t Validation loss: 117.669\n",
            "[621] Training loss: 118.715\t Validation loss: 117.135\n",
            "[622] Training loss: 118.199\t Validation loss: 116.601\n",
            "[623] Training loss: 117.682\t Validation loss: 116.066\n",
            "[624] Training loss: 117.164\t Validation loss: 115.530\n",
            "[625] Training loss: 116.646\t Validation loss: 114.994\n",
            "[626] Training loss: 116.127\t Validation loss: 114.457\n",
            "[627] Training loss: 115.607\t Validation loss: 113.919\n",
            "[628] Training loss: 115.087\t Validation loss: 113.381\n",
            "[629] Training loss: 114.565\t Validation loss: 112.843\n",
            "[630] Training loss: 114.044\t Validation loss: 112.303\n",
            "[631] Training loss: 113.521\t Validation loss: 111.764\n",
            "[632] Training loss: 112.998\t Validation loss: 111.224\n",
            "[633] Training loss: 112.474\t Validation loss: 110.683\n",
            "[634] Training loss: 111.951\t Validation loss: 110.142\n",
            "[635] Training loss: 111.425\t Validation loss: 109.601\n",
            "[636] Training loss: 110.901\t Validation loss: 109.059\n",
            "[637] Training loss: 110.373\t Validation loss: 108.518\n",
            "[638] Training loss: 109.850\t Validation loss: 107.974\n",
            "[639] Training loss: 109.320\t Validation loss: 107.433\n",
            "[640] Training loss: 108.797\t Validation loss: 106.889\n",
            "[641] Training loss: 108.265\t Validation loss: 106.348\n",
            "[642] Training loss: 107.744\t Validation loss: 105.802\n",
            "[643] Training loss: 107.207\t Validation loss: 105.263\n",
            "[644] Training loss: 106.691\t Validation loss: 104.713\n",
            "[645] Training loss: 106.147\t Validation loss: 104.178\n",
            "[646] Training loss: 105.639\t Validation loss: 103.624\n",
            "[647] Training loss: 105.084\t Validation loss: 103.095\n",
            "[648] Training loss: 104.590\t Validation loss: 102.532\n",
            "[649] Training loss: 104.015\t Validation loss: 102.015\n",
            "[650] Training loss: 103.552\t Validation loss: 101.437\n",
            "[651] Training loss: 102.938\t Validation loss: 100.944\n",
            "[652] Training loss: 102.535\t Validation loss: 100.337\n",
            "[653] Training loss: 101.851\t Validation loss: 99.892\n",
            "[654] Training loss: 101.576\t Validation loss: 99.234\n",
            "[655] Training loss: 100.773\t Validation loss: 98.897\n",
            "[656] Training loss: 100.787\t Validation loss: 98.152\n",
            "[657] Training loss: 99.821\t Validation loss: 98.075\n",
            "[658] Training loss: 100.565\t Validation loss: 97.225\n",
            "[659] Training loss: 99.581\t Validation loss: 97.861\n",
            "[660] Training loss: 102.217\t Validation loss: 97.045\n",
            "[661] Training loss: 102.062\t Validation loss: 99.634\n",
            "[662] Training loss: 107.900\t Validation loss: 99.020\n",
            "[663] Training loss: 107.750\t Validation loss: 104.001\n",
            "[664] Training loss: 111.762\t Validation loss: 100.942\n",
            "[665] Training loss: 105.437\t Validation loss: 102.833\n",
            "[666] Training loss: 105.174\t Validation loss: 97.331\n",
            "[667] Training loss: 97.980\t Validation loss: 96.984\n",
            "[668] Training loss: 98.193\t Validation loss: 93.387\n",
            "[669] Training loss: 93.939\t Validation loss: 93.207\n",
            "[670] Training loss: 94.531\t Validation loss: 91.230\n",
            "[671] Training loss: 92.090\t Validation loss: 91.146\n",
            "[672] Training loss: 92.534\t Validation loss: 89.880\n",
            "[673] Training loss: 90.931\t Validation loss: 89.747\n",
            "[674] Training loss: 91.167\t Validation loss: 88.785\n",
            "[675] Training loss: 89.958\t Validation loss: 88.588\n",
            "[676] Training loss: 90.043\t Validation loss: 87.769\n",
            "[677] Training loss: 89.031\t Validation loss: 87.526\n",
            "[678] Training loss: 89.025\t Validation loss: 86.779\n",
            "[679] Training loss: 88.113\t Validation loss: 86.512\n",
            "[680] Training loss: 88.061\t Validation loss: 85.799\n",
            "[681] Training loss: 87.194\t Validation loss: 85.526\n",
            "[682] Training loss: 87.132\t Validation loss: 84.824\n",
            "[683] Training loss: 86.273\t Validation loss: 84.562\n",
            "[684] Training loss: 86.236\t Validation loss: 83.853\n",
            "[685] Training loss: 85.353\t Validation loss: 83.622\n",
            "[686] Training loss: 85.383\t Validation loss: 82.888\n",
            "[687] Training loss: 84.446\t Validation loss: 82.719\n",
            "[688] Training loss: 84.606\t Validation loss: 81.939\n",
            "[689] Training loss: 83.582\t Validation loss: 81.882\n",
            "[690] Training loss: 83.978\t Validation loss: 81.033\n",
            "[691] Training loss: 82.840\t Validation loss: 81.177\n",
            "[692] Training loss: 83.670\t Validation loss: 80.237\n",
            "[693] Training loss: 82.432\t Validation loss: 80.763\n",
            "[694] Training loss: 84.041\t Validation loss: 79.726\n",
            "[695] Training loss: 82.796\t Validation loss: 80.944\n",
            "[696] Training loss: 85.570\t Validation loss: 79.792\n",
            "[697] Training loss: 84.272\t Validation loss: 81.966\n",
            "[698] Training loss: 87.798\t Validation loss: 80.382\n",
            "[699] Training loss: 85.434\t Validation loss: 82.849\n",
            "[700] Training loss: 87.959\t Validation loss: 80.150\n",
            "[701] Training loss: 83.705\t Validation loss: 81.577\n",
            "[702] Training loss: 84.863\t Validation loss: 78.260\n",
            "[703] Training loss: 80.311\t Validation loss: 78.803\n",
            "[704] Training loss: 81.129\t Validation loss: 76.025\n",
            "[705] Training loss: 77.573\t Validation loss: 76.325\n",
            "[706] Training loss: 78.363\t Validation loss: 74.297\n",
            "[707] Training loss: 75.776\t Validation loss: 74.530\n",
            "[708] Training loss: 76.483\t Validation loss: 73.005\n",
            "[709] Training loss: 74.512\t Validation loss: 73.186\n",
            "[710] Training loss: 75.121\t Validation loss: 71.942\n",
            "[711] Training loss: 73.499\t Validation loss: 72.088\n",
            "[712] Training loss: 74.047\t Validation loss: 70.993\n",
            "[713] Training loss: 72.609\t Validation loss: 71.134\n",
            "[714] Training loss: 73.156\t Validation loss: 70.107\n",
            "[715] Training loss: 71.801\t Validation loss: 70.284\n",
            "[716] Training loss: 72.422\t Validation loss: 69.272\n",
            "[717] Training loss: 71.082\t Validation loss: 69.539\n",
            "[718] Training loss: 71.875\t Validation loss: 68.504\n",
            "[719] Training loss: 70.507\t Validation loss: 68.937\n",
            "[720] Training loss: 71.606\t Validation loss: 67.845\n",
            "[721] Training loss: 70.189\t Validation loss: 68.556\n",
            "[722] Training loss: 71.763\t Validation loss: 67.378\n",
            "[723] Training loss: 70.286\t Validation loss: 68.504\n",
            "[724] Training loss: 72.447\t Validation loss: 67.184\n",
            "[725] Training loss: 70.800\t Validation loss: 68.777\n",
            "[726] Training loss: 73.333\t Validation loss: 67.167\n",
            "[727] Training loss: 71.149\t Validation loss: 68.955\n",
            "[728] Training loss: 73.410\t Validation loss: 66.843\n",
            "[729] Training loss: 70.386\t Validation loss: 68.308\n",
            "[730] Training loss: 71.976\t Validation loss: 65.797\n",
            "[731] Training loss: 68.524\t Validation loss: 66.758\n",
            "[732] Training loss: 69.668\t Validation loss: 64.309\n",
            "[733] Training loss: 66.475\t Validation loss: 64.963\n",
            "[734] Training loss: 67.456\t Validation loss: 62.873\n",
            "[735] Training loss: 64.795\t Validation loss: 63.395\n",
            "[736] Training loss: 65.706\t Validation loss: 61.669\n",
            "[737] Training loss: 63.514\t Validation loss: 62.133\n",
            "[738] Training loss: 64.384\t Validation loss: 60.665\n",
            "[739] Training loss: 62.513\t Validation loss: 61.111\n",
            "[740] Training loss: 63.380\t Validation loss: 59.800\n",
            "[741] Training loss: 61.704\t Validation loss: 60.269\n",
            "[742] Training loss: 62.625\t Validation loss: 59.037\n",
            "[743] Training loss: 61.052\t Validation loss: 59.579\n",
            "[744] Training loss: 62.105\t Validation loss: 58.370\n",
            "[745] Training loss: 60.572\t Validation loss: 59.048\n",
            "[746] Training loss: 61.849\t Validation loss: 57.816\n",
            "[747] Training loss: 60.310\t Validation loss: 58.703\n",
            "[748] Training loss: 61.893\t Validation loss: 57.405\n",
            "[749] Training loss: 60.297\t Validation loss: 58.560\n",
            "[750] Training loss: 62.194\t Validation loss: 57.138\n",
            "[751] Training loss: 60.429\t Validation loss: 58.537\n",
            "[752] Training loss: 62.473\t Validation loss: 56.904\n",
            "[753] Training loss: 60.357\t Validation loss: 58.378\n",
            "[754] Training loss: 62.243\t Validation loss: 56.462\n",
            "[755] Training loss: 59.692\t Validation loss: 57.773\n",
            "[756] Training loss: 61.226\t Validation loss: 55.652\n",
            "[757] Training loss: 58.440\t Validation loss: 56.694\n",
            "[758] Training loss: 59.682\t Validation loss: 54.582\n",
            "[759] Training loss: 56.988\t Validation loss: 55.415\n",
            "[760] Training loss: 58.075\t Validation loss: 53.483\n",
            "[761] Training loss: 55.662\t Validation loss: 54.200\n",
            "[762] Training loss: 56.686\t Validation loss: 52.493\n",
            "[763] Training loss: 54.572\t Validation loss: 53.160\n",
            "[764] Training loss: 55.582\t Validation loss: 51.638\n",
            "[765] Training loss: 53.706\t Validation loss: 52.302\n",
            "[766] Training loss: 54.746\t Validation loss: 50.905\n",
            "[767] Training loss: 53.029\t Validation loss: 51.608\n",
            "[768] Training loss: 54.150\t Validation loss: 50.278\n",
            "[769] Training loss: 52.527\t Validation loss: 51.067\n",
            "[770] Training loss: 53.784\t Validation loss: 49.756\n",
            "[771] Training loss: 52.200\t Validation loss: 50.677\n",
            "[772] Training loss: 53.640\t Validation loss: 49.339\n",
            "[773] Training loss: 52.040\t Validation loss: 50.425\n",
            "[774] Training loss: 53.668\t Validation loss: 49.016\n",
            "[775] Training loss: 51.976\t Validation loss: 50.256\n",
            "[776] Training loss: 53.715\t Validation loss: 48.724\n",
            "[777] Training loss: 51.835\t Validation loss: 50.040\n",
            "[778] Training loss: 53.533\t Validation loss: 48.345\n",
            "[779] Training loss: 51.404\t Validation loss: 49.611\n",
            "[780] Training loss: 52.925\t Validation loss: 47.773\n",
            "[781] Training loss: 50.605\t Validation loss: 48.898\n",
            "[782] Training loss: 51.920\t Validation loss: 47.011\n",
            "[783] Training loss: 49.572\t Validation loss: 47.986\n",
            "[784] Training loss: 50.740\t Validation loss: 46.165\n",
            "[785] Training loss: 48.513\t Validation loss: 47.034\n",
            "[786] Training loss: 49.607\t Validation loss: 45.344\n",
            "[787] Training loss: 47.567\t Validation loss: 46.158\n",
            "[788] Training loss: 48.641\t Validation loss: 44.604\n",
            "[789] Training loss: 46.783\t Validation loss: 45.406\n",
            "[790] Training loss: 47.879\t Validation loss: 43.960\n",
            "[791] Training loss: 46.164\t Validation loss: 44.788\n",
            "[792] Training loss: 47.321\t Validation loss: 43.412\n",
            "[793] Training loss: 45.701\t Validation loss: 44.301\n",
            "[794] Training loss: 46.953\t Validation loss: 42.954\n",
            "[795] Training loss: 45.381\t Validation loss: 43.933\n",
            "[796] Training loss: 46.752\t Validation loss: 42.579\n",
            "[797] Training loss: 45.177\t Validation loss: 43.662\n",
            "[798] Training loss: 46.657\t Validation loss: 42.264\n",
            "[799] Training loss: 45.023\t Validation loss: 43.438\n",
            "[800] Training loss: 46.558\t Validation loss: 41.960\n",
            "[801] Training loss: 44.805\t Validation loss: 43.173\n",
            "[802] Training loss: 46.308\t Validation loss: 41.597\n",
            "[803] Training loss: 44.409\t Validation loss: 42.780\n",
            "[804] Training loss: 45.805\t Validation loss: 41.119\n",
            "[805] Training loss: 43.794\t Validation loss: 42.219\n",
            "[806] Training loss: 45.060\t Validation loss: 40.528\n",
            "[807] Training loss: 43.025\t Validation loss: 41.533\n",
            "[808] Training loss: 44.188\t Validation loss: 39.875\n",
            "[809] Training loss: 42.218\t Validation loss: 40.807\n",
            "[810] Training loss: 43.323\t Validation loss: 39.228\n",
            "[811] Training loss: 41.469\t Validation loss: 40.119\n",
            "[812] Training loss: 42.559\t Validation loss: 38.630\n",
            "[813] Training loss: 40.829\t Validation loss: 39.512\n",
            "[814] Training loss: 41.937\t Validation loss: 38.102\n",
            "[815] Training loss: 40.313\t Validation loss: 39.005\n",
            "[816] Training loss: 41.469\t Validation loss: 37.648\n",
            "[817] Training loss: 39.920\t Validation loss: 38.596\n",
            "[818] Training loss: 41.144\t Validation loss: 37.265\n",
            "[819] Training loss: 39.633\t Validation loss: 38.274\n",
            "[820] Training loss: 40.931\t Validation loss: 36.940\n",
            "[821] Training loss: 39.420\t Validation loss: 38.014\n",
            "[822] Training loss: 40.776\t Validation loss: 36.651\n",
            "[823] Training loss: 39.221\t Validation loss: 37.771\n",
            "[824] Training loss: 40.595\t Validation loss: 36.358\n",
            "[825] Training loss: 38.964\t Validation loss: 37.491\n",
            "[826] Training loss: 40.304\t Validation loss: 36.019\n",
            "[827] Training loss: 38.588\t Validation loss: 37.125\n",
            "[828] Training loss: 39.854\t Validation loss: 35.608\n",
            "[829] Training loss: 38.080\t Validation loss: 36.659\n",
            "[830] Training loss: 39.263\t Validation loss: 35.130\n",
            "[831] Training loss: 37.483\t Validation loss: 36.122\n",
            "[832] Training loss: 38.600\t Validation loss: 34.619\n",
            "[833] Training loss: 36.865\t Validation loss: 35.564\n",
            "[834] Training loss: 37.944\t Validation loss: 34.113\n",
            "[835] Training loss: 36.287\t Validation loss: 35.032\n",
            "[836] Training loss: 37.358\t Validation loss: 33.642\n",
            "[837] Training loss: 35.785\t Validation loss: 34.558\n",
            "[838] Training loss: 36.873\t Validation loss: 33.221\n",
            "[839] Training loss: 35.374\t Validation loss: 34.154\n",
            "[840] Training loss: 36.497\t Validation loss: 32.855\n",
            "[841] Training loss: 35.051\t Validation loss: 33.819\n",
            "[842] Training loss: 36.219\t Validation loss: 32.539\n",
            "[843] Training loss: 34.799\t Validation loss: 33.543\n",
            "[844] Training loss: 36.010\t Validation loss: 32.261\n",
            "[845] Training loss: 34.586\t Validation loss: 33.301\n",
            "[846] Training loss: 35.824\t Validation loss: 32.000\n",
            "[847] Training loss: 34.370\t Validation loss: 33.061\n",
            "[848] Training loss: 35.606\t Validation loss: 31.729\n",
            "[849] Training loss: 34.104\t Validation loss: 32.788\n",
            "[850] Training loss: 35.308\t Validation loss: 31.423\n",
            "[851] Training loss: 33.759\t Validation loss: 32.457\n",
            "[852] Training loss: 34.910\t Validation loss: 31.072\n",
            "[853] Training loss: 33.335\t Validation loss: 32.068\n",
            "[854] Training loss: 34.433\t Validation loss: 30.684\n",
            "[855] Training loss: 32.864\t Validation loss: 31.642\n",
            "[856] Training loss: 33.920\t Validation loss: 30.279\n",
            "[857] Training loss: 32.387\t Validation loss: 31.208\n",
            "[858] Training loss: 33.421\t Validation loss: 29.882\n",
            "[859] Training loss: 31.942\t Validation loss: 30.797\n",
            "[860] Training loss: 32.973\t Validation loss: 29.512\n",
            "[861] Training loss: 31.551\t Validation loss: 30.426\n",
            "[862] Training loss: 32.595\t Validation loss: 29.178\n",
            "[863] Training loss: 31.225\t Validation loss: 30.104\n",
            "[864] Training loss: 32.293\t Validation loss: 28.882\n",
            "[865] Training loss: 30.957\t Validation loss: 29.829\n",
            "[866] Training loss: 32.053\t Validation loss: 28.620\n",
            "[867] Training loss: 30.734\t Validation loss: 29.590\n",
            "[868] Training loss: 31.852\t Validation loss: 28.380\n",
            "[869] Training loss: 30.529\t Validation loss: 29.368\n",
            "[870] Training loss: 31.655\t Validation loss: 28.147\n",
            "[871] Training loss: 30.312\t Validation loss: 29.141\n",
            "[872] Training loss: 31.428\t Validation loss: 27.902\n",
            "[873] Training loss: 30.057\t Validation loss: 28.887\n",
            "[874] Training loss: 31.145\t Validation loss: 27.633\n",
            "[875] Training loss: 29.752\t Validation loss: 28.598\n",
            "[876] Training loss: 30.803\t Validation loss: 27.337\n",
            "[877] Training loss: 29.401\t Validation loss: 28.275\n",
            "[878] Training loss: 30.417\t Validation loss: 27.020\n",
            "[879] Training loss: 29.026\t Validation loss: 27.933\n",
            "[880] Training loss: 30.015\t Validation loss: 26.697\n",
            "[881] Training loss: 28.653\t Validation loss: 27.592\n",
            "[882] Training loss: 29.630\t Validation loss: 26.382\n",
            "[883] Training loss: 28.306\t Validation loss: 27.268\n",
            "[884] Training loss: 29.281\t Validation loss: 26.088\n",
            "[885] Training loss: 27.998\t Validation loss: 26.974\n",
            "[886] Training loss: 28.983\t Validation loss: 25.819\n",
            "[887] Training loss: 27.733\t Validation loss: 26.713\n",
            "[888] Training loss: 28.733\t Validation loss: 25.576\n",
            "[889] Training loss: 27.507\t Validation loss: 26.482\n",
            "[890] Training loss: 28.522\t Validation loss: 25.355\n",
            "[891] Training loss: 27.306\t Validation loss: 26.272\n",
            "[892] Training loss: 28.330\t Validation loss: 25.146\n",
            "[893] Training loss: 27.112\t Validation loss: 26.070\n",
            "[894] Training loss: 28.135\t Validation loss: 24.938\n",
            "[895] Training loss: 26.906\t Validation loss: 25.861\n",
            "[896] Training loss: 27.917\t Validation loss: 24.721\n",
            "[897] Training loss: 26.673\t Validation loss: 25.633\n",
            "[898] Training loss: 27.661\t Validation loss: 24.487\n",
            "[899] Training loss: 26.408\t Validation loss: 25.383\n",
            "[900] Training loss: 27.371\t Validation loss: 24.238\n",
            "[901] Training loss: 26.118\t Validation loss: 25.114\n",
            "[902] Training loss: 27.057\t Validation loss: 23.977\n",
            "[903] Training loss: 25.816\t Validation loss: 24.837\n",
            "[904] Training loss: 26.738\t Validation loss: 23.715\n",
            "[905] Training loss: 25.520\t Validation loss: 24.563\n",
            "[906] Training loss: 26.434\t Validation loss: 23.461\n",
            "[907] Training loss: 25.243\t Validation loss: 24.303\n",
            "[908] Training loss: 26.157\t Validation loss: 23.222\n",
            "[909] Training loss: 24.994\t Validation loss: 24.064\n",
            "[910] Training loss: 25.913\t Validation loss: 23.001\n",
            "[911] Training loss: 24.774\t Validation loss: 23.846\n",
            "[912] Training loss: 25.701\t Validation loss: 22.797\n",
            "[913] Training loss: 24.578\t Validation loss: 23.647\n",
            "[914] Training loss: 25.511\t Validation loss: 22.606\n",
            "[915] Training loss: 24.396\t Validation loss: 23.461\n",
            "[916] Training loss: 25.332\t Validation loss: 22.421\n",
            "[917] Training loss: 24.216\t Validation loss: 23.277\n",
            "[918] Training loss: 25.147\t Validation loss: 22.237\n",
            "[919] Training loss: 24.025\t Validation loss: 23.087\n",
            "[920] Training loss: 24.944\t Validation loss: 22.045\n",
            "[921] Training loss: 23.817\t Validation loss: 22.885\n",
            "[922] Training loss: 24.719\t Validation loss: 21.842\n",
            "[923] Training loss: 23.589\t Validation loss: 22.670\n",
            "[924] Training loss: 24.473\t Validation loss: 21.631\n",
            "[925] Training loss: 23.347\t Validation loss: 22.444\n",
            "[926] Training loss: 24.215\t Validation loss: 21.414\n",
            "[927] Training loss: 23.101\t Validation loss: 22.215\n",
            "[928] Training loss: 23.957\t Validation loss: 21.197\n",
            "[929] Training loss: 22.860\t Validation loss: 21.990\n",
            "[930] Training loss: 23.711\t Validation loss: 20.987\n",
            "[931] Training loss: 22.634\t Validation loss: 21.775\n",
            "[932] Training loss: 23.483\t Validation loss: 20.788\n",
            "[933] Training loss: 22.426\t Validation loss: 21.575\n",
            "[934] Training loss: 23.278\t Validation loss: 20.601\n",
            "[935] Training loss: 22.238\t Validation loss: 21.388\n",
            "[936] Training loss: 23.093\t Validation loss: 20.425\n",
            "[937] Training loss: 22.065\t Validation loss: 21.214\n",
            "[938] Training loss: 22.922\t Validation loss: 20.257\n",
            "[939] Training loss: 21.899\t Validation loss: 21.046\n",
            "[940] Training loss: 22.755\t Validation loss: 20.093\n",
            "[941] Training loss: 21.734\t Validation loss: 20.879\n",
            "[942] Training loss: 22.583\t Validation loss: 19.928\n",
            "[943] Training loss: 21.561\t Validation loss: 20.708\n",
            "[944] Training loss: 22.400\t Validation loss: 19.758\n",
            "[945] Training loss: 21.376\t Validation loss: 20.529\n",
            "[946] Training loss: 22.203\t Validation loss: 19.582\n",
            "[947] Training loss: 21.180\t Validation loss: 20.343\n",
            "[948] Training loss: 21.994\t Validation loss: 19.400\n",
            "[949] Training loss: 20.976\t Validation loss: 20.150\n",
            "[950] Training loss: 21.778\t Validation loss: 19.216\n",
            "[951] Training loss: 20.771\t Validation loss: 19.957\n",
            "[952] Training loss: 21.565\t Validation loss: 19.033\n",
            "[953] Training loss: 20.570\t Validation loss: 19.767\n",
            "[954] Training loss: 21.360\t Validation loss: 18.856\n",
            "[955] Training loss: 20.380\t Validation loss: 19.585\n",
            "[956] Training loss: 21.168\t Validation loss: 18.685\n",
            "[957] Training loss: 20.203\t Validation loss: 19.413\n",
            "[958] Training loss: 20.991\t Validation loss: 18.523\n",
            "[959] Training loss: 20.039\t Validation loss: 19.249\n",
            "[960] Training loss: 20.827\t Validation loss: 18.369\n",
            "[961] Training loss: 19.883\t Validation loss: 19.093\n",
            "[962] Training loss: 20.671\t Validation loss: 18.219\n",
            "[963] Training loss: 19.733\t Validation loss: 18.942\n",
            "[964] Training loss: 20.518\t Validation loss: 18.072\n",
            "[965] Training loss: 19.582\t Validation loss: 18.790\n",
            "[966] Training loss: 20.360\t Validation loss: 17.924\n",
            "[967] Training loss: 19.425\t Validation loss: 18.636\n",
            "[968] Training loss: 20.196\t Validation loss: 17.772\n",
            "[969] Training loss: 19.262\t Validation loss: 18.476\n",
            "[970] Training loss: 20.022\t Validation loss: 17.617\n",
            "[971] Training loss: 19.091\t Validation loss: 18.313\n",
            "[972] Training loss: 19.842\t Validation loss: 17.459\n",
            "[973] Training loss: 18.917\t Validation loss: 18.146\n",
            "[974] Training loss: 19.659\t Validation loss: 17.300\n",
            "[975] Training loss: 18.742\t Validation loss: 17.980\n",
            "[976] Training loss: 19.478\t Validation loss: 17.142\n",
            "[977] Training loss: 18.572\t Validation loss: 17.816\n",
            "[978] Training loss: 19.304\t Validation loss: 16.988\n",
            "[979] Training loss: 18.409\t Validation loss: 17.658\n",
            "[980] Training loss: 19.139\t Validation loss: 16.839\n",
            "[981] Training loss: 18.254\t Validation loss: 17.506\n",
            "[982] Training loss: 18.983\t Validation loss: 16.696\n",
            "[983] Training loss: 18.108\t Validation loss: 17.360\n",
            "[984] Training loss: 18.836\t Validation loss: 16.558\n",
            "[985] Training loss: 17.968\t Validation loss: 17.220\n",
            "[986] Training loss: 18.694\t Validation loss: 16.423\n",
            "[987] Training loss: 17.831\t Validation loss: 17.081\n",
            "[988] Training loss: 18.553\t Validation loss: 16.290\n",
            "[989] Training loss: 17.693\t Validation loss: 16.943\n",
            "[990] Training loss: 18.410\t Validation loss: 16.156\n",
            "[991] Training loss: 17.552\t Validation loss: 16.803\n",
            "[992] Training loss: 18.262\t Validation loss: 16.020\n",
            "[993] Training loss: 17.406\t Validation loss: 16.661\n",
            "[994] Training loss: 18.109\t Validation loss: 15.882\n",
            "[995] Training loss: 17.257\t Validation loss: 16.515\n",
            "[996] Training loss: 17.952\t Validation loss: 15.742\n",
            "[997] Training loss: 17.105\t Validation loss: 16.369\n",
            "[998] Training loss: 17.794\t Validation loss: 15.602\n",
            "[999] Training loss: 16.954\t Validation loss: 16.223\n",
            "[1000] Training loss: 17.638\t Validation loss: 15.463\n"
          ]
        }
      ]
    }
  ]
}